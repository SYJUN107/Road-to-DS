### Transformer_Architecture.md
- Encoder-Decoder Structure.
- Self-Attention Mechanism (Scaled Dot-Product Attention): QKV (Query, Key, Value), how attention scores are calculated.
- Multi-Head Attention: Benefits.
- Positional Encoding: Why it's needed, conceptual methods.
- Feedforward Networks, Layer Normalization, Residual Connections in Transformer.
### LLMs Foundations Architectures
- Pre-training vs. Fine-tuning paradigm.
- Common Architectures: BERT (encoder-only, masked language modeling), GPT (decoder-only, causal language modeling), T5 (encoder-decoder, text-to-text framework).
- Tokenization (Byte-Pair Encoding, WordPiece).
- Embeddings (Word2Vec, GloVe, FastText, Contextual Embeddings from LLMs).
### LLM FineTuning Prompt Engineering
- Full Fine-tuning vs. Parameter-Efficient Fine-Tuning (PEFT): LoRA, QLoRA, Adapter tuning (conceptual).
- Prompt Engineering: Few-shot, zero-shot, Chain-of-Thought (CoT), Tree-of-Thought, RAG (Retrieval-Augmented Generation).
- When to use different prompt styles for different tasks (summarization, Q&A, generation).
- In-context learning.
### LLM Evaluation Safety
- Evaluation Metrics: Perplexity, BLEU, ROUGE, BERTScore, human evaluation.
- Benchmarks: GLUE, SuperGLUE, MMLU.
- Hallucinations, Toxicity, Bias.
- Alignment (RLHF - conceptual).
- Model Safety and Responsible Use.