### Core Concepts
- Perceptron, Multi-Layer Perceptron (MLP).
- Activation Functions: ReLU and its variants (Leaky, ELU, GELU), Sigmoid, Tanh, Softmax (when to use).
- Loss Functions: Cross-entropy, MSE, Custom loss functions.
- Backpropagation: Conceptual understanding (chain rule application, gradient flow).
- Optimization Algorithms: SGD, Momentum, Adagrad, RMSProp, Adam (conceptual differences, pros/cons, learning rate considerations).
---

### Regularization & Normalization
- L1/L2 Regularization (weight decay).
- Dropout: How it works, benefits, practical usage.
- Batch Normalization: Why it helps, how it works (mean, variance normalization), inference vs. training behavior.
- Layer Normalization, Instance Normalization, Group Normalization (conceptual differences).
---

### Training Techniques
- Learning Rate Schedulers: Step decay, cosine annealing, ReduceLROnPlateau.
- Gradient Clipping: Why and when to use.
- Early Stopping: When and how to apply.
- Initialization Strategies (Xavier, He).
- Handling Vanishing/Exploding Gradients.
- ---

